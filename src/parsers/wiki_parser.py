#реализовать википарсер из коллаба (последняя ячейка)
#def wiki_parser(url: str, base_path: str) -> List[str]: #аннотации(структурирование вывода функции) перменной/функции
  '''
  функция возвращает словарь
  библиотеки: BeautifulSoup - работа с html
              requests - проверка доступа к сайту
              uuid
  мапы - замена циклов
  бинарные файлы - файлы .bin, оперируют байтами
  метод .split для вычленения слов
          избавимся от знаков препинаний
          с помощью сплита структурируем данные
  base_path - путь к каталогу, в котором хранится файл(words или бинарный content)
  в контенте хранится весь текст
  в бинарном файле отформатированный текст

  '''
  import bs4
  import requests
  import uuid
  import os

  url = input()

  current_dir = os.getcwd() #адрес текущей директории в абс формате

  def wiki_parser(url: str, base_path: str) -> List1[str]:
    file_list = os.listdir(current_dir) #лист директорий внутри текущей
    f = open('url.txt', 'w')
    a = f.open
    for b in a:


  """
  0) пробегается по директориям в base_path, сравнивает содержимое файла url.txt
     с параметром url. Если такая найдена то сразу переходим в пункт 2)
  1) в директории по пути base_path создает папку
      со случайно сгенерированным именем. Для генерации можно использовать
      import uuid
      dirname = uuid.uuid4().hex
  2) если папка существовала и в ней уже есть файл content с контентом страницы то её читает
     иначе загружает и записывает в бинарный файл content всё содержимое страницы
  3) из контента вытаскивает текст, считает слова с помощью Map
  4) сериализует мапу в текcтовый файл words.txt в папке
  5) из контента вытаскивает ссылки, фильтрует оставляя только ссылки на викиепедию
     и возвращает список
     Upd: base_path - аргумент, а не константа
     для возможности работы с url, они будут хранится в отдельном файле внутри папки
  """

